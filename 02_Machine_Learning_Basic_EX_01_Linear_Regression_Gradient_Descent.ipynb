{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gharashim/DL/blob/main/02_Machine_Learning_Basic_EX_01_Linear_Regression_Gradient_Descent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FBMFIOU_oWOg"
      },
      "source": [
        "# Linear Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w690xaWJoaQU"
      },
      "source": [
        ">### [예제 1] Gradient Descent of Linear Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zuxRw2vAo5Fi"
      },
      "source": [
        ">### Load modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LNPBqbLgGfUs",
        "outputId": "f21cb265-9985-4eaf-a673-40ceb8b23a54"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NumPy Version :1.25.2\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "print(\"NumPy Version :{}\".format(np.__version__))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OctX9GUDJ6Z3"
      },
      "source": [
        "> ### Input and Label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YNt07NUKKx6Y"
      },
      "outputs": [],
      "source": [
        "# Input and Labels\n",
        "x_input = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10], dtype= np.float32)\n",
        "labels = np.array([3, 5, 7, 9, 11, 13, 15, 17, 19, 21], dtype= np.float32)\n",
        "\n",
        "# Weights\n",
        "W = np.random.normal()\n",
        "B = np.random.normal()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VaUmHQgGA8-d"
      },
      "source": [
        ">### Hypothesis : Linear Equation\n",
        ">### $h(x) = wx + b$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "td6Qr3VuNFug"
      },
      "outputs": [],
      "source": [
        "# Hypothesis : Linear Function\n",
        "def Hypothesis(x):\n",
        "    return W*x + B"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BFpxFy_0pMRz"
      },
      "source": [
        ">### Cost Function : Mean Squared Error (MSE)\n",
        ">### $\\sum_{i=1}^{n}(h(x_{i})-y_{i})^{2}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Szvt5sniBQiJ"
      },
      "outputs": [],
      "source": [
        "# Cost : Mean Squared Error\n",
        "def Cost():\n",
        "    return np.mean((Hypothesis(x_input) - labels)**2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uxyl6ifpBJfZ"
      },
      "source": [
        ">### Gradient\n",
        ">### $\\frac{\\partial}{\\partial w}cost(w, b) = \\frac{1}{m}  \\sum_{i=1}^{m}(x_{i}(x_{i}w+(b-y_{i})))$\n",
        ">### $\\frac{\\partial}{\\partial b}cost(w, b) = \\frac{1}{m}  \\sum_{i=1}^{m}(x_{i}w - y_{i} + b)$\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "urqcydR4shiM"
      },
      "outputs": [],
      "source": [
        "def Gradient(x, y):\n",
        "    return np.mean(x*(x*W+(B-y))), np.mean((W*x-y+B))\n",
        "\n",
        "# def Gradient(x, y):\n",
        "#     global W, B\n",
        "#     pres_w, pres_b = W, B # W,B backup\n",
        "#     delta = 5e-7\n",
        "\n",
        "#     W = pres_w + delta\n",
        "#     cost_p = Cost()\n",
        "#     W = pres_w - delta\n",
        "#     cost_m = Cost()\n",
        "#     grad_w = (cost_p-cost_m)/(2*delta)\n",
        "#     W = pres_w # w restore\n",
        "\n",
        "#     B = pres_b + delta\n",
        "#     cost_p = Cost()\n",
        "#     B = pres_b - delta\n",
        "#     cost_m = Cost()\n",
        "#     grad_b = (cost_p-cost_m)/(2*delta)\n",
        "\n",
        "#     B = pres_b # b restore\n",
        "#     return grad_w, grad_b"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-qwbFA4-wQFE"
      },
      "source": [
        ">### Training\n",
        ">### $\\mu$ : Learning rate\n",
        ">### $w = w - \\mu\\frac{\\partial}{\\partial w}cost(w, b)$\n",
        ">### $b = b - \\mu\\frac{\\partial}{\\partial b}cost(w, b)$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tzaB4hcauxx-",
        "outputId": "3ee0f745-948b-4aaa-e575-c5928c5d8a9d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[    0] cost =      113.2, W =  0.2656, B =   1.136\n",
            "[  250] cost =    0.01803, W =   1.958, B =    1.29\n",
            "[  500] cost =    0.01066, W =   1.968, B =   1.223\n",
            "[  750] cost =   0.006306, W =   1.975, B =   1.172\n",
            "[ 1000] cost =    0.00373, W =   1.981, B =   1.132\n",
            "[ 1250] cost =   0.002206, W =   1.985, B =   1.101\n",
            "[ 1500] cost =   0.001305, W =   1.989, B =   1.078\n",
            "[ 1750] cost =  0.0007715, W =   1.991, B =    1.06\n",
            "[ 2000] cost =  0.0004563, W =   1.993, B =   1.046\n",
            "[ 2250] cost =  0.0002699, W =   1.995, B =   1.035\n",
            "[ 2500] cost =  0.0001596, W =   1.996, B =   1.027\n",
            "[ 2750] cost =   9.44e-05, W =   1.997, B =   1.021\n",
            "[ 3000] cost =  5.583e-05, W =   1.998, B =   1.016\n",
            "[ 3250] cost =  3.302e-05, W =   1.998, B =   1.012\n",
            "[ 3500] cost =  1.953e-05, W =   1.999, B =    1.01\n",
            "[ 3750] cost =  1.155e-05, W =   1.999, B =   1.007\n",
            "[ 4000] cost =  6.831e-06, W =   1.999, B =   1.006\n",
            "[ 4250] cost =   4.04e-06, W =   1.999, B =   1.004\n",
            "[ 4500] cost =  2.389e-06, W =     2.0, B =   1.003\n",
            "[ 4750] cost =  1.413e-06, W =     2.0, B =   1.003\n",
            "[ 5000] cost =  8.361e-07, W =     2.0, B =   1.002\n",
            "CPU times: user 206 ms, sys: 0 ns, total: 206 ms\n",
            "Wall time: 212 ms\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "# Parameter Set\n",
        "epochs = 5000\n",
        "learning_rate = 0.005\n",
        "\n",
        "# 학습 (Training)\n",
        "for cnt in range(0, epochs+1):\n",
        "    if cnt % (epochs//20) == 0:\n",
        "      print(\"[{:>5}] cost = {:>10.4}, W = {:>7.4}, B = {:>7.4}\".format(cnt, Cost(), W, B))\n",
        "\n",
        "    grad_w, grad_b = Gradient(x_input, labels)\n",
        "    W -= learning_rate * grad_w\n",
        "    B -= learning_rate * grad_b"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}